# STF Scraper

Uma biblioteca Python robusta e modular para fazer scraping de dados de processos judiciais do Supremo Tribunal Federal (STF) com suporte a armazenamento em Parquet e integraÃ§Ã£o com basedosdados.org.

## ğŸš€ CaracterÃ­sticas Principais

- **EstratÃ©gia HÃ­brida**: Consulta primeiro o dataset "Corte Aberta" via basedosdados.org, com fallback para scraping direto
- **Formato Parquet**: Armazenamento eficiente com Polars em modo lazy
- **Suporte S3**: PersistÃªncia local ou em buckets Amazon S3
- **ResiliÃªncia**: Retry automÃ¡tico com exponential backoff, tratamento de rate limiting
- **Processamento Paralelo**: Multi-threading para maior performance
- **Checkpoint System**: RecuperaÃ§Ã£o de progresso em caso de interrupÃ§Ã£o
- **ExtraÃ§Ã£o de PDF**: Suporte completo para documentos PDF usando PyMuPDF e pdfplumber
- **Anti-bloqueio**: RotaÃ§Ã£o de user-agents, suporte a proxies, delays configurÃ¡veis

## ğŸ“¦ InstalaÃ§Ã£o

```bash
pip install stf-scraper
```

### InstalaÃ§Ã£o para Desenvolvimento

```bash
git clone https://github.com/stf-scraper/stf-scraper.git
cd stf-scraper
pip install -e ".[dev]"
```

## ğŸ¯ Uso BÃ¡sico

```python
from stf_scraper import STFScraper

# Lista de processos no formato CNJ
processos = [
    "1234567-89.2023.1.00.0000",
    "9876543-21.2022.1.00.0000"
]

# Criar instÃ¢ncia do scraper
scraper = STFScraper(
    process_list=processos,
    output_path="processos_stf.parquet",  # ou "s3://meu-bucket/processos.parquet"
    batch_size=500,
    max_retries=5
)

# Executar scraping
resultado = scraper.run()

print(f"Processados: {resultado['total_records']} processos")
print(f"Taxa de sucesso: {resultado['success_rate']:.1f}%")
```

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Uso com Proxies

```python
proxies = [
    "proxy1.exemplo.com:8080",
    "proxy2.exemplo.com:8080"
]

scraper = STFScraper(
    process_list=processos,
    output_path="s3://meu-bucket/processos.parquet",
    use_proxies=True,
    proxy_list=proxies,
    max_workers=10,
    rate_limit_delay=2.0
)
```

### ConfiguraÃ§Ã£o Completa

```python
scraper = STFScraper(
    process_list=processos,
    output_path="processos_stf.parquet",

    # ConfiguraÃ§Ãµes de performance
    batch_size=1000,
    max_workers=8,
    rate_limit_delay=1.5,

    # ConfiguraÃ§Ãµes de resiliÃªncia
    max_retries=10,
    checkpoint_interval=50,

    # ConfiguraÃ§Ãµes de rede
    use_proxies=True,
    proxy_list=proxies,
    use_headless_browser=True,

    # ConfiguraÃ§Ãµes de fonte de dados
    use_basedosdados=True,

    # ConfiguraÃ§Ãµes de log
    log_file="stf_scraper.log",
    log_level="INFO"
)

resultado = scraper.run()
```

## ğŸ“Š Estrutura dos Dados de SaÃ­da

O arquivo Parquet gerado contÃ©m as seguintes colunas:

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `processo_numero` | String | NÃºmero do processo no formato CNJ |
| `classe_processual` | String | Classe do processo |
| `assunto` | String | Assunto/matÃ©ria do processo |
| `relator` | String | Ministro relator |
| `origem` | String | Ã“rgÃ£o de origem |
| `data_autuacao` | String | Data de autuaÃ§Ã£o |
| `status` | String | Status atual do processo |
| `partes` | String | JSON com informaÃ§Ãµes das partes |
| `movimentacoes` | String | JSON com movimentaÃ§Ãµes |
| `documentos` | String | JSON com lista de documentos |
| `decisoes` | String | JSON com decisÃµes |
| `texto_integral` | String | Texto completo extraÃ­do |
| `fonte_dados` | String | Fonte: 'basedados', 'scraping', 'cache' |
| `data_extracao` | String | Timestamp da extraÃ§Ã£o |
| `sucesso_extracao` | Boolean | Se a extraÃ§Ã£o foi bem-sucedida |

## ğŸ”§ Componentes da Biblioteca

### STFScraper (Orquestrador Principal)
Classe principal que coordena todo o processo de extraÃ§Ã£o.

### RequestManager (Gerenciador de RequisiÃ§Ãµes)
- Retry automÃ¡tico com exponential backoff
- Suporte a proxies e rotaÃ§Ã£o de user-agents
- Rate limiting inteligente
- Suporte a Selenium para casos complexos

### HTMLParser (Parser HTML)
- ExtraÃ§Ã£o estruturada de metadados
- IdentificaÃ§Ã£o automÃ¡tica de padrÃµes do STF
- Limpeza e normalizaÃ§Ã£o de texto

### PDFExtractor (ExtraÃ§Ã£o de PDF)
- Suporte a PyMuPDF e pdfplumber
- DetecÃ§Ã£o automÃ¡tica de PDFs escaneados
- PreservaÃ§Ã£o de layout e estrutura

### DataManager (Gerenciamento de Dados)
- Armazenamento eficiente em Parquet
- Suporte nativo ao S3
- Sistema de checkpoint para recuperaÃ§Ã£o
- Processamento em batches

## ğŸ” Monitoramento e Logs

A biblioteca oferece logging detalhado e barras de progresso:

```python
# Configurar logging personalizado
scraper = STFScraper(
    process_list=processos,
    output_path="processos.parquet",
    log_file="scraping.log",
    log_level="DEBUG"  # DEBUG, INFO, WARNING, ERROR
)

# Durante execuÃ§Ã£o, vocÃª verÃ¡:
# - Barra de progresso em tempo real
# - EstatÃ­sticas de fontes de dados
# - Taxa de sucesso/erro
# - Tempo estimado restante
```

## ğŸ›¡ï¸ Tratamento de Erros e ResiliÃªncia

### Rate Limiting
```python
# A biblioteca trata automaticamente HTTP 429
# e respeita headers Retry-After
scraper = STFScraper(
    process_list=processos,
    output_path="processos.parquet",
    rate_limit_delay=2.0,  # Delay base entre requisiÃ§Ãµes
    max_retries=10
)
```

### Checkpoint e RecuperaÃ§Ã£o
```python
# Em caso de interrupÃ§Ã£o, execute novamente
# A biblioteca continuarÃ¡ de onde parou
scraper = STFScraper(
    process_list=processos,
    output_path="processos.parquet",
    checkpoint_interval=100  # Salva checkpoint a cada 100 processos
)

resultado = scraper.run()  # Continua automaticamente
```

## ğŸŒ IntegraÃ§Ã£o com Base dos Dados

A biblioteca tenta primeiro buscar dados no dataset "Corte Aberta" da Base dos Dados:

```python
# Configure seu projeto do Google Cloud
import os
os.environ['GOOGLE_CLOUD_PROJECT'] = 'seu-projeto-gcp'

scraper = STFScraper(
    process_list=processos,
    output_path="processos.parquet",
    use_basedosdados=True  # PadrÃ£o: True
)
```

## ğŸ“ˆ Performance e Escalabilidade

Para grandes volumes (15.000+ processos):

```python
scraper = STFScraper(
    process_list=processos_grandes,
    output_path="s3://bucket/processos.parquet",

    # ConfiguraÃ§Ãµes otimizadas
    batch_size=1000,
    max_workers=15,
    checkpoint_interval=50,

    # Use proxies para evitar bloqueios
    use_proxies=True,
    proxy_list=lista_proxies,
    rate_limit_delay=0.5
)
```

## ğŸ§ª Exemplos AvanÃ§ados

### Processamento com Filtros Personalizados

```python
from stf_scraper import STFScraper
from stf_scraper.utils.validators import CNJValidator

# Filtrar apenas processos vÃ¡lidos
processos_brutos = ["123.456", "1234567-89.2023.1.00.0000", "invÃ¡lido"]
processos_validos, _ = CNJValidator.validate_process_list(processos_brutos)

scraper = STFScraper(
    process_list=processos_validos,
    output_path="processos_filtrados.parquet"
)
```

### AnÃ¡lise PÃ³s-Processamento

```python
import polars as pl

# Carregar dados processados
df = pl.read_parquet("processos_stf.parquet")

# EstatÃ­sticas bÃ¡sicas
print(f"Total de processos: {len(df)}")
print(f"Processos com texto: {df.filter(pl.col('tamanho_texto') > 0).height}")

# AnÃ¡lise por relator
relatores = (df
    .group_by('relator')
    .agg(pl.count().alias('quantidade'))
    .sort('quantidade', descending=True)
)
print(relatores.head(10))
```

## ğŸš¦ LimitaÃ§Ãµes e Boas PrÃ¡ticas

### LimitaÃ§Ãµes
- Depende da estrutura atual do portal STF
- PDFs escaneados requerem OCR (feature futura)
- Rate limiting do portal pode afetar velocidade

### Boas PrÃ¡ticas
- Use proxies para volumes grandes
- Configure delays adequados (1-2 segundos)
- Monitore logs para detectar bloqueios
- Use checkpoints para processamentos longos
- Prefira horÃ¡rios de menor trÃ¡fego

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o repositÃ³rio
2. Crie uma branch para sua feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ†˜ Suporte

- ğŸ“§ Email: contact@stfscraper.dev
- ğŸ› Issues: [GitHub Issues](https://github.com/stf-scraper/stf-scraper/issues)
- ğŸ“š DocumentaÃ§Ã£o: [ReadTheDocs](https://stf-scraper.readthedocs.io/)

## ğŸ† CrÃ©ditos

Desenvolvido com â¤ï¸ para a comunidade jurÃ­dica brasileira.

---

**Aviso Legal**: Este software Ã© fornecido "como estÃ¡" e destina-se apenas a fins educacionais e de pesquisa. Os usuÃ¡rios sÃ£o responsÃ¡veis por cumprir os termos de uso do portal STF e todas as leis aplicÃ¡veis.
