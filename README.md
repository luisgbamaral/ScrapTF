# STF Scraper

Uma biblioteca Python robusta e modular para fazer scraping de dados de processos judiciais do Supremo Tribunal Federal (STF) com suporte a armazenamento em Parquet e integra√ß√£o com basedosdados.org.

<img src="https://github.com/luisgbamaral/ScrapTF/blob/main/ScraperTF.png">

## üöÄ Caracter√≠sticas Principais

- **Estrat√©gia H√≠brida**: Consulta primeiro o dataset "Corte Aberta" via basedosdados.org, com fallback para scraping direto
- **Formato Parquet**: Armazenamento eficiente com Polars em modo lazy
- **Suporte S3**: Persist√™ncia local ou em buckets Amazon S3
- **Resili√™ncia**: Retry autom√°tico com exponential backoff, tratamento de rate limiting
- **Processamento Paralelo**: Multi-threading para maior performance
- **Checkpoint System**: Recupera√ß√£o de progresso em caso de interrup√ß√£o
- **Extra√ß√£o de PDF**: Suporte completo para documentos PDF usando PyMuPDF e pdfplumber
- **Anti-bloqueio**: Rota√ß√£o de user-agents, suporte a proxies, delays configur√°veis

## üì¶ Instala√ß√£o

```bash
pip install stf-scraper
```

### Instala√ß√£o para Desenvolvimento

```bash
git clone https://github.com/stf-scraper/stf-scraper.git
cd stf-scraper
pip install -e ".[dev]"
```

## üéØ Uso B√°sico

```python
from stf_scraper import STFScraper

# Lista de processos no formato CNJ
processos = [
    "1234567-89.2023.1.00.0000",
    "9876543-21.2022.1.00.0000"
]

# Criar inst√¢ncia do scraper
scraper = STFScraper(
    process_list=processos,
    output_path="processos_stf.parquet",  # ou "s3://meu-bucket/processos.parquet"
    batch_size=500,
    max_retries=5
)

# Executar scraping
resultado = scraper.run()

print(f"Processados: {resultado['total_records']} processos")
print(f"Taxa de sucesso: {resultado['success_rate']:.1f}%")
```

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

### Uso com Proxies

```python
proxies = [
    "proxy1.exemplo.com:8080",
    "proxy2.exemplo.com:8080"
]

scraper = STFScraper(
    process_list=processos,
    output_path="s3://meu-bucket/processos.parquet",
    use_proxies=True,
    proxy_list=proxies,
    max_workers=10,
    rate_limit_delay=2.0
)
```

### Configura√ß√£o Completa

```python
scraper = STFScraper(
    process_list=processos,
    output_path="processos_stf.parquet",

    # Configura√ß√µes de performance
    batch_size=1000,
    max_workers=8,
    rate_limit_delay=1.5,

    # Configura√ß√µes de resili√™ncia
    max_retries=10,
    checkpoint_interval=50,

    # Configura√ß√µes de rede
    use_proxies=True,
    proxy_list=proxies,
    use_headless_browser=True,

    # Configura√ß√µes de fonte de dados
    use_basedosdados=True,

    # Configura√ß√µes de log
    log_file="stf_scraper.log",
    log_level="INFO"
)

resultado = scraper.run()
```

## üìä Estrutura dos Dados de Sa√≠da

O arquivo Parquet gerado cont√©m as seguintes colunas:

| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| `processo_numero` | String | N√∫mero do processo no formato CNJ |
| `classe_processual` | String | Classe do processo |
| `assunto` | String | Assunto/mat√©ria do processo |
| `relator` | String | Ministro relator |
| `origem` | String | √ìrg√£o de origem |
| `data_autuacao` | String | Data de autua√ß√£o |
| `status` | String | Status atual do processo |
| `partes` | String | JSON com informa√ß√µes das partes |
| `movimentacoes` | String | JSON com movimenta√ß√µes |
| `documentos` | String | JSON com lista de documentos |
| `decisoes` | String | JSON com decis√µes |
| `texto_integral` | String | Texto completo extra√≠do |
| `fonte_dados` | String | Fonte: 'basedados', 'scraping', 'cache' |
| `data_extracao` | String | Timestamp da extra√ß√£o |
| `sucesso_extracao` | Boolean | Se a extra√ß√£o foi bem-sucedida |

## üîß Componentes da Biblioteca

### STFScraper (Orquestrador Principal)
Classe principal que coordena todo o processo de extra√ß√£o.

### RequestManager (Gerenciador de Requisi√ß√µes)
- Retry autom√°tico com exponential backoff
- Suporte a proxies e rota√ß√£o de user-agents
- Rate limiting inteligente
- Suporte a Selenium para casos complexos

### HTMLParser (Parser HTML)
- Extra√ß√£o estruturada de metadados
- Identifica√ß√£o autom√°tica de padr√µes do STF
- Limpeza e normaliza√ß√£o de texto

### PDFExtractor (Extra√ß√£o de PDF)
- Suporte a PyMuPDF e pdfplumber
- Detec√ß√£o autom√°tica de PDFs escaneados
- Preserva√ß√£o de layout e estrutura

### DataManager (Gerenciamento de Dados)
- Armazenamento eficiente em Parquet
- Suporte nativo ao S3
- Sistema de checkpoint para recupera√ß√£o
- Processamento em batches

## üîç Monitoramento e Logs

A biblioteca oferece logging detalhado e barras de progresso:

```python
# Configurar logging personalizado
scraper = STFScraper(
    process_list=processos,
    output_path="processos.parquet",
    log_file="scraping.log",
    log_level="DEBUG"  # DEBUG, INFO, WARNING, ERROR
)

# Durante execu√ß√£o, voc√™ ver√°:
# - Barra de progresso em tempo real
# - Estat√≠sticas de fontes de dados
# - Taxa de sucesso/erro
# - Tempo estimado restante
```

## üõ°Ô∏è Tratamento de Erros e Resili√™ncia

### Rate Limiting
```python
# A biblioteca trata automaticamente HTTP 429
# e respeita headers Retry-After
scraper = STFScraper(
    process_list=processos,
    output_path="processos.parquet",
    rate_limit_delay=2.0,  # Delay base entre requisi√ß√µes
    max_retries=10
)
```

### Checkpoint e Recupera√ß√£o
```python
# Em caso de interrup√ß√£o, execute novamente
# A biblioteca continuar√° de onde parou
scraper = STFScraper(
    process_list=processos,
    output_path="processos.parquet",
    checkpoint_interval=100  # Salva checkpoint a cada 100 processos
)

resultado = scraper.run()  # Continua automaticamente
```

## üåê Integra√ß√£o com Base dos Dados

A biblioteca tenta primeiro buscar dados no dataset "Corte Aberta" da Base dos Dados:

```python
# Configure seu projeto do Google Cloud
import os
os.environ['GOOGLE_CLOUD_PROJECT'] = 'seu-projeto-gcp'

scraper = STFScraper(
    process_list=processos,
    output_path="processos.parquet",
    use_basedosdados=True  # Padr√£o: True
)
```

## üìà Performance e Escalabilidade

Para grandes volumes (15.000+ processos):

```python
scraper = STFScraper(
    process_list=processos_grandes,
    output_path="s3://bucket/processos.parquet",

    # Configura√ß√µes otimizadas
    batch_size=1000,
    max_workers=15,
    checkpoint_interval=50,

    # Use proxies para evitar bloqueios
    use_proxies=True,
    proxy_list=lista_proxies,
    rate_limit_delay=0.5
)
```

## üß™ Exemplos Avan√ßados

### Processamento com Filtros Personalizados

```python
from stf_scraper import STFScraper
from stf_scraper.utils.validators import CNJValidator

# Filtrar apenas processos v√°lidos
processos_brutos = ["123.456", "1234567-89.2023.1.00.0000", "inv√°lido"]
processos_validos, _ = CNJValidator.validate_process_list(processos_brutos)

scraper = STFScraper(
    process_list=processos_validos,
    output_path="processos_filtrados.parquet"
)
```

### An√°lise P√≥s-Processamento

```python
import polars as pl

# Carregar dados processados
df = pl.read_parquet("processos_stf.parquet")

# Estat√≠sticas b√°sicas
print(f"Total de processos: {len(df)}")
print(f"Processos com texto: {df.filter(pl.col('tamanho_texto') > 0).height}")

# An√°lise por relator
relatores = (df
    .group_by('relator')
    .agg(pl.count().alias('quantidade'))
    .sort('quantidade', descending=True)
)
print(relatores.head(10))
```

## üö¶ Limita√ß√µes e Boas Pr√°ticas

### Limita√ß√µes
- Depende da estrutura atual do portal STF
- PDFs escaneados requerem OCR (feature futura)
- Rate limiting do portal pode afetar velocidade

### Boas Pr√°ticas
- Use proxies para volumes grandes
- Configure delays adequados (1-2 segundos)
- Monitore logs para detectar bloqueios
- Use checkpoints para processamentos longos
- Prefira hor√°rios de menor tr√°fego

## ü§ù Contribui√ß√£o

1. Fork o reposit√≥rio
2. Crie uma branch para sua feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

**Aviso Legal**: Este software √© fornecido "como est√°" e destina-se apenas a fins educacionais e de pesquisa. Os usu√°rios s√£o respons√°veis por cumprir os termos de uso do portal STF e todas as leis aplic√°veis.
