# ScrapTF

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-stable-brightgreen.svg)

**Biblioteca Python otimizada para extraÃ§Ã£o automatizada de dados de processos do Supremo Tribunal Federal (STF).**

<img src="https://github.com/luisgbamaral/ScrapTF/blob/main/ScraperTF.png">

## ğŸš€ InstalaÃ§Ã£o

```bash
pip install stf-scraper
```

### InstalaÃ§Ã£o manual
```bash
git clone https://github.com/seu-usuario/stf-scraper.git
cd stf-scraper
pip install -e .
```

## ğŸ“‹ Uso BÃ¡sico

### Python
```python
from stf_scraper import STFScraper

# Criar scraper
scraper = STFScraper()

# Extrair processos
processos = ["0001234-56.2023.1.00.0000", "0001235-56.2023.1.00.0000"]
df = scraper.scrape_processes(processos, output_file="processos.parquet")

print(f"ExtraÃ­dos {len(df)} processos")
```

### CLI
```bash
# Extrair processos especÃ­ficos
stf-scraper 0001234-56.2023.1.00.0000 0001235-56.2023.1.00.0000

# Extrair de arquivo
stf-scraper lista_processos.txt -o resultados.csv

# Configurar workers e delay
stf-scraper processos.txt -w 3 -d 1.5
```

## ğŸ“Š Dados ExtraÃ­dos

A biblioteca extrai automaticamente:

- **Dados bÃ¡sicos**: nÃºmero, classe, relator, assunto, status
- **Partes**: requerentes, requeridos, interessados
- **MovimentaÃ§Ãµes**: histÃ³rico processual com datas
- **Texto integral**: decisÃµes, ementas e conteÃºdo completo
- **Metadados**: timestamps e status de extraÃ§Ã£o

## âš™ï¸ ConfiguraÃ§Ãµes

```python
scraper = STFScraper(
    max_workers=2,        # Workers paralelos (padrÃ£o: 2)
    rate_limit_delay=2.0, # Delay entre requisiÃ§Ãµes (padrÃ£o: 2.0s)
    max_retries=3,        # Tentativas por processo (padrÃ£o: 3)
    timeout=30            # Timeout requisiÃ§Ãµes (padrÃ£o: 30s)
)
```

## ğŸ” ValidaÃ§Ã£o CNJ

```python
from stf_scraper import CNJValidator

# Validar nÃºmero
valido = CNJValidator.validate_cnj_number("0001234-56.2023.1.00.0000")

# Formatar nÃºmero
formatado = CNJValidator.format_cnj_number("00012345620231000000")
# Resultado: "0001234-56.2023.1.00.0000"
```

## ğŸ“ Formatos de SaÃ­da

Suporte automÃ¡tico baseado na extensÃ£o do arquivo:

- `.parquet` - Recomendado (eficiente e rÃ¡pido)
- `.csv` - CompatÃ­vel com Excel
- `.json` - Estrutura preservada
- `.xlsx` - Excel nativo

## ğŸ”§ Exemplos AvanÃ§ados

### ExtraÃ§Ã£o em Lote
```python
# Ler processos de arquivo
with open('processos.txt', 'r') as f:
    processos = [linha.strip() for linha in f]

# Configurar para lote grande
scraper = STFScraper(max_workers=3, rate_limit_delay=1.5)

# Executar
df = scraper.scrape_processes(processos, output_file="lote.parquet")

# EstatÃ­sticas
stats = scraper.get_stats()
print(f"Taxa de sucesso: {stats['success']/stats['total']*100:.1f}%")
```

### AnÃ¡lise de Texto
```python
# Extrair processo especÃ­fico
dados = scraper.scrape_single_process("0001234-56.2023.1.00.0000")

# Analisar texto integral
if dados.get('texto_integral'):
    texto = dados['texto_integral']
    print(f"Tamanho: {len(texto)} caracteres")

    # Buscar palavras-chave
    keywords = ['inconstitucional', 'precedente']
    for keyword in keywords:
        count = texto.lower().count(keyword)
        print(f"{keyword}: {count} ocorrÃªncias")
```

## ğŸ›¡ï¸ Boas PrÃ¡ticas

### Performance
- Use `max_workers=2-3` para evitar sobrecarga
- Configure `rate_limit_delay >= 1.5` segundos
- Prefira formato Parquet para grandes volumes

### Responsabilidade
- Respeite os limites do servidor STF
- Monitore estatÃ­sticas de erro
- Use delays adequados

## ğŸ“‹ Requisitos

- Python 3.8+
- requests >= 2.28.0
- beautifulsoup4 >= 4.11.0
- pandas >= 1.5.0
- lxml >= 4.9.0
- tqdm >= 4.64.0

## ğŸ§ª Testes

```bash
# Executar testes
pytest tests/

# Teste rÃ¡pido
python examples/exemplo_basico.py
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro de SSL
```python
import urllib3
urllib3.disable_warnings()
```

### Timeout
```python
scraper = STFScraper(timeout=60)  # Aumentar timeout
```

### Grandes Volumes
```python
# Processar em lotes
for i in range(0, len(processos), 100):
    lote = processos[i:i+100]
    df = scraper.scrape_processes(lote)
    df.to_parquet(f"lote_{i//100}.parquet")
```

## ğŸ“„ LicenÃ§a

MIT License - Veja [LICENSE](LICENSE) para detalhes.

## âš–ï¸ Disclaimer

Esta biblioteca Ã© para fins educacionais e de pesquisa. Respeite os termos de uso do portal STF.

---

**Desenvolvido para a comunidade jurÃ­dica brasileira** ğŸ‡§ğŸ‡·
